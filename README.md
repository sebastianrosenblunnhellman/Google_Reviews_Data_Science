<h1 align="center">üçΩÔ∏è YELP & GOOGLE MAPS REVIEWS  üó∫Ô∏è</h1>

## üìã **Tabla de contenidos**
- [Contexto](#Contexto)
- [Objetivos](#Objetivos)
- [MPV](#MVP)
- [Alcance del proyecto](#Alcance-del-proyecto)
- [Limitaciones del proyecto](#Limitaciones-del-proyecto)
- [KPI's](#KPI¬¥s)
- [Metodolog√≠a de Trabajo](#metodologia-del-trabajo)
- [Stack Tecnol√≥gico](#Stack-Tecnol√≥gico)

<!-- Contexto section -->
##  **Contexto**

Como parte de una consultora de data, nos han contratado para poder realizar un an√°lisis del mercado estadounidense. Nuestro cliente es parte de un conglomerado de empresas de restaurantes y afines, y desean tener un an√°lisis detallado de la opini√≥n de los usuarios en Yelp y cruzarlos con los de Google Maps sobre hoteles, restaurantes y otros negocios afines al turismo y ocio, utilizando an√°lisis de sentimientos, predecir cu√°les ser√°n los rubros de los negocios que m√°s crecer√°n o decaer√°n. Adem√°s, desean saber d√≥nde es conveniente emplazar los nuevos locales de restaurantes y afines, y desean poder tener un sistema de recomendaci√≥n de restaurantes para los usuarios de ambas plataformas para darle al usuario, por ejemplo, la posibilidad de poder conocer nuevos sabores basados en sus experiencias previas.

<!-- objetivos section -->
## **üéØ Objetivos**
### **üåü Objetivo General 1**

Desarrollar un sistema de recomendaci√≥n de restaurantes que satisfaga las necesidades y preferencias de los usuarios.   

### **üîç Objetivos espec√≠ficos**

1. Recomendar restaurantes similares a otros que hayan tenido √©xito utilizando t√©cnicas de filtrado colaborativo.

2. Recomendar restaurantes que se ajusten a los gustos y preferencias de los usuarios utilizando modelos basados en contenido.

### **üåü Objetivo General 2**
Identificar la zona m√°s conveniente para emplazar nuevos locales mediante el an√°lisis geoespacial y t√©cnicas de correlaci√≥n, con el fin de tomar decisiones estrat√©gicas que optimicen el rendimiento de un posible nuevo negocio.

### **üîç Objetivos espec√≠ficos**
1. Analizar datos de ubicaci√≥n geogr√°fica, demogr√°ficos para identificar variables que puedan correlacionarse con el √©xito de un negocio.

2. Evaluar las relaciones de correlaci√≥n entre estas variables y el rendimiento pasado de negocios similares en diferentes √°reas geogr√°ficas.

3. Seleccionar la zona m√°s prometedora para la expansi√≥n del negocio bas√°ndose en las correlaciones identificadas.

### **üåü Objetivo General 3**
Utilizar t√©cnicas de an√°lisis de datos para comprender mejor el comportamiento futuro del mercado en un rubro tur√≠stico dado.

### **üîç Objetivos espec√≠ficos**
1. Determinar las variables que m√°s influyen en el crecimiento o decrecimiento de los negocios.

2. Identificar tendencias en las ventas o ingresos de diferentes rubros de negocios mediante el an√°lisis de series temporales con modelos ARIMA.

### **üåü Objetivos Comunes**

1. **Extracci√≥n de datos desde la fuente:** Utilizaremos las API¬¥s provistas por Yelp y google maps y datos recolecci√≥n propia/scrapping para las variables demogr√°ficas.

2. **Disponibilizar datos en la nube:** Implementaremos un proceso de carga incremental con servicios de GCP. Asi los datos podr√°n ser consumidos por nuestra plataforma.

3. **Limpieza de Datos:** Corregir valores at√≠picos, datos faltantes y normalizar los datos, asegurando la integridad de los mismos antes de la an√°lisis.

4. **Automatizaci√≥n:** Automatizar tanto como sea posible el proceso de ETL para mejorar la eficiencia y reducir el riesgo de errores manuales.

5. **Documentaci√≥n:** Documentar detalladamente todo el proceso de ETL, incluyendo las fuentes de datos, las transformaciones realizadas y los criterios de calidad aplicados, para facilitar la replicabilidad y el mantenimiento del proceso.

<p align="right">(<a href="#readme-top">volver arriba</a>)</p>

<!-- mpv section -->
## **üöÄ MPV**  

El Producto M√≠nimo Viable (MVP) debe ser una Interfaz de usuario  que proporciona las caracter√≠sticas elaboradas en los objetivos.
La interfaz de usuario deber√° ser intuitiva y f√°cil de usar, permitiendo a los usuarios acceder a estas funcionalidades de manera clara y efectiva. Adem√°s, deber√° ser capaz de mostrar los resultados de manera visualmente atractiva, mediante gr√°ficos, mapas y listas de recomendaciones.


<!-- Alcance section -->
## **üåê Alcance del proyecto**

**Extracci√≥n de Datos:** Recopilar y utilizar datos de plataformas de rese√±as como Google Maps y Yelp.

**An√°lisis de Datos:** El sistema utilizar√° t√©cnicas avanzadas de Analisis de Datos y Aprendizaje Autom√°tico  para analizar las rese√±as de los usuarios. Este an√°lisis permitir√° al sistema identificar tendencias y realizar recomendaciones consistentemente.

**Cobertura Geogr√°fica:** El sistema se centrar√° espec√≠ficamente en los locales de comida en los estados de California, Florida y Nueva York en los Estados Unidos.

**Visualizaci√≥n y Sistema de Recomendaci√≥n:** El sistema proporcionar√° una interfaz de usuario intuitiva que permite a nuestros clientes seleccionar su criterio de restaurantes, visualizar los resultados de la recomendaci√≥n y proporcionar retroalimentaci√≥n.

**Iteraci√≥n y Mejora:** Con la retroalimentaci√≥n de los usuarios, el sistema ser√° iterado y mejorado continuamente para satisfacer mejor las necesidades de los usuarios.

<!-- Fuera de alcance section -->
## **‚ùå Limitaciones del proyecto**

**Datos en tiempo real:** Dependiendo de las limitaciones de las API de Google Maps y Yelp, puede que no podamos obtener datos en tiempo real. Esto podr√≠a afectar la capacidad del sistema para identificar las tendencias m√°s recientes.

**Personalizaci√≥n profunda:** Si bien podemos personalizar las recomendaciones bas√°ndonos en las rese√±as, puede que no podamos personalizarlas en funci√≥n de las preferencias diet√©ticas individuales o las restricciones alimentarias de los usuarios por falta de datos.

**Escalabilidad:** Aunque el sistema est√° dise√±ado para escalar y cubrir otras regiones, la implementaci√≥n inicial se limitar√° a California, Florida y Nueva York. La expansi√≥n geogr√°fica podr√≠a estar fuera del alcance inicial del proyecto. Lo mismo vale para extrapolar el modelo a otros rubros comerciales o √°reas afines.

‚ÄÉ<!-- KPI section -->
## **üìä KPI¬¥s:**

‚Ä¢ **Crecimiento de rese√±as positivas:** 
* **Descripci√≥n**: Este KPI se centra en el aumento porcentual del n√∫mero de rese√±as positivas en comparaci√≥n del a√±o anterior.
* **Objetivo**: Aumentar 5% las rese√±as positivas para los negocios en comparaci√≥n con el a√±o anterior.

$$
\mathrm{KPI} = \frac{R_{\text{a√±oActual}}^{+} - R_{\text{a√±oAnterior}}^{+}}{R_{\text{a√±oAnterior}}^{+}} \cdot 100
$$

<br>

‚Ä¢ **Disminuci√≥n de rese√±as negativas:** 
* **Descripci√≥n**:Este KPI se centra en la disminuci√≥n porcentual del n√∫mero de rese√±as negativas en comparaci√≥n del a√±o anterior.
* **Objetivo**: Disminuir 5%  la tasa de rese√±as en comparaci√≥n con el a√±o anterior.

$$
\mathrm{KPI} = \frac{R_{\text{a√±oAnterior}}^{-} - R_{\text{a√±oActual}}^{-}}{R_{\text{a√±oAnterior}}^{-}} \cdot 100
$$

<br>

‚Ä¢ **Aumento de la tasa anual de retenci√≥n de usuarios:** 
* **Descripci√≥n**:Mide la tasa de usuarios que escriben rese√±as a√±o a a√±o.
* **Objetivo**: Aumentar 5%  la tasa de rese√±as en comparaci√≥n con el a√±o anterior.

$$
\mathrm{KPI} = \frac{U_{\text{Rese√±asActual}}- U_{\text{Rese√±asAnterior}}}{U_{\text{Rese√±asAnterior}}}
$$

<br>

<!-- flujo section -->
## **üîß Flujo de Trabajo**

Lo primero que realizamos al recibir los datos en crudo es un trabajo de etl manual y estandarizado a traves de python y las librerias pertinentes. Este etl inicial consta de la eliminacion de las columnas irrelevantes, si es necesario desanidamos columnas, manejamos los valores nulos y duplicados, normalizamos tipos de datos y los nombres de las columnas para estandarizar segun el esquema que se adapta a nuestros objetivos y finalmente se segmentamos las tablas segun corresponda.

![ETL inicial](assets/etl_inicial.png)

A partir de aca los datos sigues dos caminos diferentes y vuelven a encontrar en el producto final. Por un lado tenemos los datos que seran untilizados para entrenar los sistemas de recomendaci√≥nn por filtrado colaborativo y  filtrado basado en contenido.  Los mismos se implementaran sobre fast api, utilizando streamlit como interface y google app engine para el deploy. Por otro lado, en el segundo camino de los datos, Luego del etl inicial, los datos pasan a un bucket especifico de google cloud storage donde funcionan como trigger para el inicio de las funciones de google cloud functions que ingestara las tablas en big query.
La funcion verifica si se trata de un archivo csv y de ser asi lo carga de forma temporal y pasa a validar si coincide con el esquema asignado para la tabla de bigquery, de ser asi comienza a subir los datos en otra tabla temporal en big query en donde se realizara un merge con la tabla original si es que se cumplen ciertas condiciones. En ambas tablas solo se cargaran los datos si no estan en tabla original, proceso conocido como carga incremental.pero la tabla reviews y business difieren en las condiciones del merge. El criterio utilizado para la tabla reviews es que que si los datos que se intentan agregar difieren en user_id y timestamp se agregan enfectivamente, asumiendo como supuesto logico que el mismo user puede haber realizado mas de una rese√±a pero nunca exatamente al mismo tiempo. En el caso de la tabla business la condicion para a√±adir los datos es que el business_id sea diferente y tambien la latitud y longitud, asumiendo que pueden haber varios locales emplazados en lugares diferentes. Finalmente una vez subidos a bigquery estos se conectan directamente con looker para disponibilizar los datos de forma integrada y utilizarlos en la construccion de un dashboard estrategico que luego sera embebido en el el producto final, esto es, la interface de usuario, encontrandose en el mismo punto los dos caminos iniciales que tomaron los datos luego del etl inicial.

Junto al codigo de las funciones en el repositorio se puede encontrar un README.md en la sub-carpeta ML de la carpeta "Sprint 3 Machine Learning y Analitics" donde se explica con precision el desarrollo y funcionamiento del modelo de Machine Learning.

![flujo de trabajo](assets/workflow.jpg)

## Diagrama E-R

![Diagrama ER detallado](assets/diagrama_e_r.jpg)

Por lo que respecta al diccionario de datos este se encuentra en el readme.md de la carpeta "Sprint 2 Ingenieria de Datos"


<!-- metodolog√≠a section -->
## **üîß Metodolog√≠a del Trabajo**

La metodolog√≠a Scrum divide el trabajo en partes peque√±as y manejables llamadas "sprints". Cada sprint dura un per√≠odo de tiempo corto, en este caso una semana, durante el cual el equipo se enfoca en completar un conjunto espec√≠fico de tareas. Al final de cada sprint, se realizar√° una demo (sprint review meeting) en la que se har√° una demostraci√≥n de los entregables desarrollados, esperando una retroalimentaci√≥n. Se ajusta as√≠ la planificaci√≥n para el siguiente sprint seg√∫n lo que se haya aprendido. Adem√°s, cada d√≠a se realizar√°n reuniones diarias de seguimiento (Daily Standup), para discutir el progreso diario y posibles inconvenientes. Todo esto permite una adaptaci√≥n continua a medida que el equipo avanza.

![metodolog√≠a scrum](assets/scrummetodo.jpg)

**Sprint 1 - Comprensi√≥n del Negocio y de los Datos**:

**Sprint 2 - Preparaci√≥n de los Datos y Modelado**: 

**Sprint 3 - Evaluaci√≥n y Despliegue**: 

<!-- stack section -->
## **üíª Stack Tecnol√≥gico**

**Visual Studio Code:** Software para trabajar de forma local en el proyecto.

- **Bases de datos:** Azure o PostgreSQL para almacenar los datos, MongoDB para datos no estructurados como rese√±as de usuarios.

- **Lenguajes de programaci√≥n:** Python para an√°lisis de datos y desarrollo de modelos de machine learning.

- **Bibliotecas y frameworks:** pandas, scikit-learn, TensorFlow / PyTorch para machine learning, Flask o FastAPI para desarrollar APIs.

- **Herramientas de visualizaci√≥n:** Matplotlib, Seaborn, Plotly para visualizaci√≥n de datos.


<!-- team section -->
## **üë• Miembros del Equipo**

| Rol            |  Nombre              | LinkedIn | GitHub |
| -------------- |--------------------- | -------- |-|
| Data Engineer  | Sebastian Rosenblunn      | [![LinkedIn][linkedin-shield]][linkedin-Naza]  | [![GitHub][github-shield]][github-Naza]  |
| Data Scientist | Nazareno Fantin | [![LinkedIn][linkedin-shield]][linkedin-Sebas] | [![GitHub][github-shield]][github-Sebas] |
| Data Scientist | Alejo Diez Gomez     | [![LinkedIn][linkedin-shield]][linkedin-Alejo]   | [![GitHub][github-shield]][github-Alejo]   |
| Data Analyst   | Keyla Serna          | [![LinkedIn][linkedin-shield]][linkedin-Keyla] | [![GitHub][github-shield]][github-Keyla] |
| Data Analyst   | Tom√°s Basovich       | [![LinkedIn][linkedin-shield]][linkedin-Tom] | [![GitHub][github-shield]][github-Tom] |
<!-- | Project Management  | Scrum Master   |  |Maximiliano Vaca Coll |       ||-->

<p align="right">(<a href="#readme-top">volver arriba</a>)</p>

<!-- MARKDOWN LINKS & IMAGES -->
<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->

[linkedin-shield]: https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white
[github-shield]: https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white

[linkedin-Naza]: https://www.linkedin.com/in/nazareno-fantin/
[linkedin-Sebas]:https://github.com/Nazario3482/Proyecto-Grupal-Google-yelp  
[linkedin-Alejo]:https://www.linkedin.com/in/alejo-gabriel-diez-gomez-402b93254/
[linkedin-Keyla]:www.linkedin.com/in/keyla-elyneth
[linkedin-Tom]:https://github.com/Nazario3482/Proyecto-Grupal-Google-

[github-Naza]: https://github.com/Nazario3482
[github-Sebas]:https://github.com/Nazario3482/Proyecto-Grupal-Google-yelp
[github-Alejo]:https://github.com/AlejoDiezGomez
[github-Keyla]:https://github.com/KeylaSernaB
[github-Tom]:https://github.com/Nazario3482/Proyecto-Grupal-Google-yelp
